apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pokemon-data-pipeline
spec:
  entrypoint: main
  arguments:
    parameters:
      - name: input-data-path
        value: "s3a://pengfei/workflow_demo/pokemon/pokemon-raw.csv"
      - name: data-path
          value: "/mnt/data"
      - name: output-file-list
        value: |
          [
            { "file-path": "/mnt/tmp", "file-name": "pokemon-enriched.csv" },
            { "file-path": "/mnt/tmp", "file-name": "random_forest_pokemon_legendary_classifier.sav" },
            { "file-path": "/mnt/tmp", "file-name": "pokemon-ml-output.txt" },
            { "file-path": "/mnt/tmp", "file-name": "top-5-pokemon.csv" }
          ]
      - name: aws-access-id
        value: ""
      - name: aws-secret-key
        value: ""
      - name: aws-session-token
        value: ""
      - name: aws-default-region
        value: "us-east-1"
      - name: aws-s3-endpoint
        value: "minio.lab.sspcloud.fr"
      # The mlflow tracking server is responsable to log the hyper-parameter and model metrics,
      # You can create it inside the datalab, and copy the url. Below is an example
      # https://pengfei-mlflow-7841853311341079041-mlflow-ihm.kub.sspcloud.fr/
      - name: mlflow-tracking-uri
        value: 'https://user-pengfei-786489.kub.sspcloud.fr'
      - name: mlflow-experiment-name
        value: "pokemon"
      - name: mlflow-s3-url
        value: "https://minio.lab.sspcloud.fr"
      - name: code-source-repo
        value: "https://github.com/pengfei99/mlflow-pokemon-example.git"
      - name: model-training-conf-list
        value: |
          [
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-partial.csv", "nestimator": 90, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-partial.csv", "nestimator": 70, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-new.csv", "nestimator": 50, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-new.csv", "nestimator": 70, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-cleaned.csv", "nestimator": 50, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-cleaned.csv", "nestimator": 70, "maxDepth": 30,"minSamplesSplit": 2} 
          ]
  # Create a pvc for the workflow
  volumeClaimTemplates:
    - metadata:
        name: pokemon-workflow-tmp
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 10Gi

  templates:
    #############################################################################################
    #################### main template for planning dag of the pipeline #########################
    #############################################################################################
    - name: main
      dag:
        tasks:
          # task 0: load code source and data
          - name: load-code-and-data
            template: load-code-and-data-wt
          # task 1: check duplicated rows
          - name: check-duplicated-rows
            dependencies: [ load-code-and-data ]
            template: check-duplicated-rows-wt
          # task 2: remove duplicated rows,this task is activated when task 1 detects duplication
          - name: remove-duplicated-rows
            dependencies: [ check-duplicated-rows]
            template: remove-duplicated-rows-wt
            when: "{{tasks.check-duplicated-rows.outputs.parameters.has-duplicated-rows}} == True"
          # task 2-1: if no duplicated rows, copy the origin input file
          - name: copy-origin-input-file
            dependencies: [ check-duplicated-rows]
            template: copy-origin-input-file-wt
            when: "{{tasks.check-duplicated-rows.outputs.parameters.has-duplicated-rows}} == False"

    ####################################################################################################################
    #################### task template for implementing the logic of each task of the pipeline #########################
    ####################################################################################################################
    # worker template for task-0 load code source and data
    - name: load-code-and-data-wt
      inputs:
        artifacts:
          # Check out the master branch of the argo repo and place it at /src
          # revision can be anything that git checkout accepts: branch, commit, tag, etc.
          - name: code
            path: /mnt/bin
            git:
              repo: https://github.com/pengfei99/WorkflowDemo.git
              revision: "main"
      container:
        image: liupengfei99/python38-ds
        command: [sh, -c]
        args: ["mkdir -p /mnt/data ; python /mnt/bin/src/data_ingestion/ingest_source_data.py {{workflow.parameters.input-data-path}} /mnt/data/; ls -l /mnt/bin /mnt/data"]
        volumeMounts:
          - name: pokemon-workflow-tmp
            mountPath: /mnt
        env:
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.aws-secret-key}}"
          - name: AWS_DEFAULT_REGION
            value: "{{workflow.parameters.aws-default-region}}"
          - name: AWS_S3_ENDPOINT
            value: "{{workflow.parameters.aws-s3-endpoint}}"
          - name: AWS_SESSION_TOKEN
            value: "{{workflow.parameters.aws-session-token}}"
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.aws-access-id}}"
          - name: PYTHONPATH
              value: "${PYTHONPATH}:/mnt/bin"
          - name: LOG_PATH
              value: "{{workflow.parameters.log-path}}"
          # worker template for task-0 load code source and data
    - name: check-duplicated-rows
      container:
        image: liupengfei99/python38-ds
        command: [ sh, -c ]
        args: [ "python /mnt/bin/src/data_quality/check_duplication.py {{workflow.parameters.input-data-path}} {{workflow.parameters.data-path}}" ]
        volumeMounts:
          - name: pokemon-workflow-tmp
            mountPath: /mnt
        outputs:
          parameters:
            - name: has-duplicated-rows
              valueFrom:
                # Default value to use if retrieving valueFrom fails. If not provided workflow will fail instead
                default: "True"
                path: /mnt/data/output-params.txt
        env:
          - name: PYTHONPATH
              value: "${PYTHONPATH}:/mnt/bin"
          - name: LOG_PATH
              value: "{{workflow.parameters.data-path}}"

    - name: remove-duplicated-rows-wt
      container:
        image: liupengfei99/python38-ds
        command: [ sh, -c ]
        args: [ "python -V && python /mnt/bin/src/data_cleaning/RemoveDuplicatedRows.py
                 /mnt/input/pokemon-bad.csv /mnt/tmp/pokemon-dedup.csv" ]
        volumeMounts:
          - name: workflow-tmp
            mountPath: /mnt

    # worker template for task-5-1 remove duplicated rows
    - name: copy-origin-input-file-wt
      container:
        image: busybox
        command: [ sh, -c ]
        args: [ "cp {{workflow.parameters.data-path}}/pokemon-raw.csv {{workflow.parameters.data-path}}/pokemon-dedup.csv" ]
        volumeMounts:
          - name: workflow-tmp
            mountPath: /mnt


