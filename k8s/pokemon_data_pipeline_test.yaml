apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pokemon-data-pipeline
spec:
  entrypoint: main
  arguments:
    parameters:
      - name: input-data-path
        value: "s3a://pengfei/workflow-demo/pokemon-raw.csv"
      - name: output-file-list
        value: |
          [
            { "file-path": "/mnt/tmp", "file-name": "pokemon-enriched.csv" },
            { "file-path": "/mnt/tmp", "file-name": "random_forest_pokemon_legendary_classifier.sav" },
            { "file-path": "/mnt/tmp", "file-name": "pokemon-ml-output.txt" },
            { "file-path": "/mnt/tmp", "file-name": "top-5-pokemon.csv" }
          ]
      - name: aws-access-id
        value: ""
      - name: aws-secret-key
        value: ""
      - name: aws-session-token
        value: ""
      - name: aws-default-region
        value: "us-east-1"
      - name: aws-s3-endpoint
        value: "minio.lab.sspcloud.fr"
      # The mlflow tracking server is responsable to log the hyper-parameter and model metrics,
      # You can create it inside the datalab, and copy the url. Below is an example
      # https://pengfei-mlflow-7841853311341079041-mlflow-ihm.kub.sspcloud.fr/
      - name: mlflow-tracking-uri
        value: 'https://user-pengfei-786489.kub.sspcloud.fr'
      - name: mlflow-experiment-name
        value: "pokemon"
      - name: mlflow-s3-url
        value: "https://minio.lab.sspcloud.fr"
      - name: code-source-repo
        value: "https://github.com/pengfei99/mlflow-pokemon-example.git"
      - name: model-training-conf-list
        value: |
          [
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-partial.csv", "nestimator": 90, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-partial.csv", "nestimator": 70, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-new.csv", "nestimator": 50, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-new.csv", "nestimator": 70, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-cleaned.csv", "nestimator": 50, "maxDepth": 30,"minSamplesSplit": 2},
            { "data": "https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-cleaned.csv", "nestimator": 70, "maxDepth": 30,"minSamplesSplit": 2} 
          ]
  # Create a pvc for the workflow
  volumeClaimTemplates:
    - metadata:
        name: pokemon-workflow-tmp
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 10Gi

  templates:
    #############################################################################################
    #################### main template for planning dag of the pipeline #########################
    #############################################################################################
    - name: main
      dag:
        tasks:
          # task 0: load code source and data
          - name: load-code-and-data
            template: load-code-and-data-wt

    ####################################################################################################################
    #################### task template for implementing the logic of each task of the pipeline #########################
    ####################################################################################################################
    # worker template for task-0 load code source and data
    - name: load-code-and-data-wt
      inputs:
        artifacts:
          # Check out the master branch of the argo repo and place it at /src
          # revision can be anything that git checkout accepts: branch, commit, tag, etc.
          - name: code
            path: /mnt/bin
            git:
              repo: https://github.com/pengfei99/WorkflowDemo.git
              revision: "main"
      container:
        image: liupengfei99/python38-ds
        command: [sh, -c]
        args: ["mkdir -p /mnt/data ; python /mnt/bin/WorkflowDemo/src/data_ingestion/ingest_source_data.py {{workflow.parameters.input-data-path}} /mnt/data/; ls -l /mnt/bin /mnt/data"]
        volumeMounts:
          - name: workflow-tmp
            mountPath: /mnt
        env:
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.aws-secret-key}}"
          - name: AWS_DEFAULT_REGION
            value: "{{workflow.parameters.aws-default-region}}"
          - name: AWS_S3_ENDPOINT
            value: "{{workflow.parameters.aws-s3-endpoint}}"
          - name: AWS_SESSION_TOKEN
            value: "{{workflow.parameters.aws-session-token}}"
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.aws-access-id}}"

